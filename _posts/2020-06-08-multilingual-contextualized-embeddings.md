---
layout: post
title: Multilinguality in Transformers Architectures.
---
**When**:  Wednesday 17th of June, 9:00 AM

**Where**: Meet, check the address in the Google Calendar Event.

**Topic**: Overview of cross lingual pretrained language models and their hidden structures.

**Speaker**: 
[Edoardo Barba](https://twitter.com/sampj94) 


### Abstract
Transfer Learning has been one of the leading actors in the recent developments of natural language processing research. In this reading group we are going to have a brief overview on how different transfer learning techniques are employed throughout the literature, with a focus on multilingual applications. We will discuss recent works on cross-lingual language models and analyze which are the factors that mostly allow multilingual pretrained contextualized embeddings to share common patterns across languages. Finally, we will explore how we can transfer knowledge from high resource languages to the low resource ones with the help of these models.

### Reccommended Readings:
- [Emerging Cross-lingual Structure in Pretrained Language Models](https://arxiv.org/pdf/1911.01464.pdf)
- [On the Cross-lingual Transferability of Monolingual Representations](https://arxiv.org/pdf/1910.11856.pdf)

### Material
- [Slide](material/2020-06-08-multilingual-contextualized-embeddings/Multilinguality in Transformers Architectures.pdf)
- [Presentation](https://drive.google.com/file/d/1NaAa-N6tisCyjxoO5pr6iTbfNhrQVrtX/view?usp=sharing)


### Questioners:
- Agostina 
- Federico (Scozzafava)
- Rocco