---
layout: post
title: Making BERT, BART, and Other Monsters Talk.
---

**When**:  Wednesday 25th of March 2020 at 09:00.

**Where**: Hangout, check the address in the Google Calendar Event.

**Topic**: Pretraining a Conditioned Generation Architecture: Making BERT, BART, and Other Monsters Talk.

**Speakers**: [Michele Bevilacqua](https://twitter.com/MicheleBevila20)
### Abstract
The recent success of models like ELMo, BERT, XLNet and the likes has shown that pretraining through self-supervision and transfer 
learning are very effective techniques for addressing discriminative NLP tasks. However, this was not the case for many tasks 
involving generation, and the use of pretrained architectures brought but modest improvements. 
Two recent and concurrent approaches, BART and T5, aim to fill the gap by evaluating sequence-to-sequence pretraining tasks, 
in which the model has to output a sequence. These models can be used for both generative and discriminative downstream applications.


### Material
- [Slides](https://sapienzanlp.github.io/reading-group/material/2020-03-20-conditioned_generation_architectures/RG_2020___Pretrained_Generation.pdf)
- [Presentation](https://drive.google.com/file/d/1hJ9ordYMR8HcTqma4Sh9VlawzVrMjxBy/view?usp=sharing)

### Reccommended Readings:
Among all the reference papers for the models, the two most recent approaches are the followings:
- [BART: Denoising Sequence-to-Sequence Pre-training for NaturalLanguage Generation, Translation, and Comprehension. Lewis et al. 2019](https://arxiv.org/pdf/1910.13461.pdf)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Raffel et al. 2019](https://arxiv.org/abs/1910.10683)

### Content
Approaches to Semantic Parsing:
- Introduction to Pretraining by Denoising.
- Introduction to Sequence to Sequence Modelling.
- Use of BERT in Machine Translation and Abstractive Summarization.
- Models Covered:
    - SpanBERT
    - UniLM
    - MASS
    - T5
    - BART
    - mBART
    
### Questioners:
- Edoardo Barba
- Luigi Procopio
- Niccol√≤ Campolungo

