---
layout: post
title: Top Pics from ACL on Knowledge-enhanced neural networks.
---
**When**:  Wednesday 29th of July, 9:00 AM

**Where**: Meet, check the address in the Google Calendar Event.

**Topic**: Beyond pre-trained models: how inserting knowledge can improve commonsense reasoning.

**Speaker**: 
[Caterina Lacerra](https://twitter.com/CaterinaLac)

### Abstract
Recently pretrained transformer models have shown good results when applied to commonsense tasks such as dialogue generation, story generation or commonsense reasoning. 
In this talk I will present three ACL works that try to improve from this  starting point including knowledge to the networks, both explicitly retrieving it from knowledge bases, and  implicitly learning it from a pretrained language model.


### Material
- [Slides](https://sapienzanlp.github.io/reading-group/material/2020-07-29-beyond-pre-trained-models/beyond_pretrained_models.pdf)
- [Presentation](https://drive.google.com/file/d/159e4llbljqBo6fQqeSKMmqsl9NsGn_rZ/view?usp=sharing)
### Recommended Readings
- [Pre-training is (Almost) All You Need: An Application to Commonsense Reasoning](https://www.aclweb.org/anthology/2020.acl-main.357.pdf) - Tamborrino et al.
- [Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness](https://www.aclweb.org/anthology/2020.acl-main.515/) - Wu et al.
- [Knowledge-Enhanced Pretraining Model for Commonsense Story Generation](https://transacl.org/ojs/index.php/tacl/article/view/1886) - Guan et al.

### Questioners:
- Tommaso Pasini
- Niccol√≤ Campolungo
- Simone Conia