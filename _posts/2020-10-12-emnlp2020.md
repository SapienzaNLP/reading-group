---
layout: post
title: Presentations rehearsal, EMNLP2020
---
**When**:  Monday 12th of October, 10:00 AM

**Where**: Meet, check the address in the Google Calendar Event.

**Topic**: Presentations for EMNLP papers.

**Speaker**: 
- [Rexhina Blloshmi](https://twitter.com/rexhina_b)
- [Marco Maru](https://twitter.com/MarcoMaru3)
- [Bianca Scarlini](https://twitter.com/biancascarlini)
- [Tommaso Pasini](https://twitter.com/pasinit)

### Material
- [XL-AMR Presentation](https://drive.google.com/file/d/1RbdqAVK-HM4AO1BM5oCGq8GBD1KkOG4J/view?usp=sharing)
- [Generationary and Ares Presentations](https://drive.google.com/file/d/1hCEohE8hTl8wnVfBmlR9Fc1QHWdIkSKu/view?usp=sharing)
### XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques
Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a crosslingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages.

### Generationary or: "How We Went beyond Sense Inventories and Learned to Gloss"
Mainstream computational lexical semantics
embraces the assumption that word senses can
be represented as discrete items of a predefined inventory. In this paper we show this
needs not be the case, and propose a unified
model that is able to produce contextually appropriate definitions. In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to
generate glosses. We show that, even though
we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many
settings, but it also matches or surpasses the
state of the art in discriminative tasks such
as Word Sense Disambiguation and Word-inContext. Finally, we show that Generationary
benefits from training on data from multiple
inventories, with strong gains on various zeroshot benchmarks, including a novel dataset
of definitions for free adjective-noun phrases.
The software and reproduction materials are
available at [http://generationary.org](http://generationary.org).

### With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation
Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures.